{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857add85-76c5-4a81-b866-1ea1fc6af309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Output:\n",
      "[[0.59045606]\n",
      " [0.67428665]\n",
      " [0.49960203]\n",
      " [0.01722848]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Group B: Assignment No. 7\n",
    "Assignment Title: Write a python program in python program for\n",
    "creating a Back Propagation Feed-forward neural network\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Define sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define input dataset\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# Define output dataset\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100000\n",
    "\n",
    "# Initialize weights randomly with mean 0\n",
    "hidden_weights = 2*np.random.random((2,2)) - 1\n",
    "output_weights = 2*np.random.random((2,1)) - 1\n",
    "\n",
    "# Train the neural network\n",
    "for i in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden_layer = sigmoid(np.dot(X, hidden_weights))\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, output_weights))\n",
    "    # Backpropagation\n",
    "    output_error = y - output_layer\n",
    "    output_delta = output_error * sigmoid_derivative(output_layer)\n",
    "    hidden_error = output_delta.dot(output_weights.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer)\n",
    "    output_weights += hidden_layer.T.dot(output_delta) * learning_rate\n",
    "    hidden_weights += X.T.dot(hidden_delta) * learning_rate\n",
    "    \n",
    "# Display input and output\n",
    "print(\"Input:\")\n",
    "print(X)\n",
    "print(\"Output:\")\n",
    "print(output_layer)\n",
    "\n",
    "'''Here are the detailed answers to your questions on **Forward and Backward Propagation** in neural networks:\n",
    "\n",
    "---\n",
    "\n",
    "### **Q1. What Is Forward and Backward Propagation?**\n",
    "\n",
    "* **Forward Propagation** is the process by which input data is passed through the neural network layer by layer to produce an output (prediction).\n",
    "* **Backward Propagation** (or backpropagation) is the process of updating the weights of the neural network using the error between the predicted and actual outputs, typically using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. How do Forward and Backward Propagation work?**\n",
    "\n",
    "* **Forward Propagation:**\n",
    "\n",
    "  * Inputs are fed into the input layer.\n",
    "  * Data flows through each hidden layer where each neuron computes a weighted sum of inputs followed by an activation function.\n",
    "  * Final output is produced in the output layer.\n",
    "\n",
    "* **Backward Propagation:**\n",
    "\n",
    "  * The loss (error) is calculated using a loss function.\n",
    "  * The gradient of the loss is computed with respect to each weight using the chain rule.\n",
    "  * These gradients are used to update the weights to minimize the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. Difference Between Forward and Backward Propagation**\n",
    "\n",
    "| Aspect      | Forward Propagation            | Backward Propagation                      |\n",
    "| ----------- | ------------------------------ | ----------------------------------------- |\n",
    "| Purpose     | Compute predictions from input | Update weights based on prediction errors |\n",
    "| Direction   | Input → Output                 | Output → Input (in reverse)               |\n",
    "| Computation | Weighted sum + activation      | Gradient calculation using chain rule     |\n",
    "| Output      | Predicted value                | Weight updates (gradients)                |\n",
    "| Trigger     | Starts with input data         | Starts with error from loss function      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What are steps involved in Forward Propagation?**\n",
    "\n",
    "1. **Input Layer:** Pass input features to the first hidden layer.\n",
    "2. **Weighted Sum (Preactivation):** Compute $Z = W \\cdot X + b$\n",
    "3. **Activation:** Apply activation function: $A = f(Z)$\n",
    "4. **Repeat for each layer:** Use the output of the current layer as input to the next.\n",
    "5. **Output Layer:** Generate the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. What are steps involved in Backward Propagation?**\n",
    "\n",
    "1. **Compute Loss:** Calculate the error using a loss function.\n",
    "2. **Output Layer Gradient:** Compute the derivative of the loss w\\.r.t output.\n",
    "3. **Propagate Error Backward:** Use the chain rule to compute gradients layer by layer.\n",
    "4. **Compute Weight Gradients:** Derivatives w\\.r.t weights and biases.\n",
    "5. **Update Parameters:** Adjust weights and biases using gradient descent:\n",
    "   $W = W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. What is Preactivation and Activation in Forward Propagation?**\n",
    "\n",
    "* **Preactivation (Z):**\n",
    "  The linear combination of inputs and weights before applying the activation function.\n",
    "  $Z = W \\cdot X + b$\n",
    "\n",
    "* **Activation (A):**\n",
    "  The result after applying the activation function to the preactivation value.\n",
    "  $A = f(Z)$, where $f$ could be ReLU, sigmoid, tanh, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like diagrams or Python code examples to visualize this process.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd91b4-4c25-4e8c-84e7-b6423841fabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
