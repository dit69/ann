{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ddf78-177c-41f1-aa02-83133113ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit | ASCII | Output | Even/Odd\n",
      "-------------------------------\n",
      "  0    |  48   |   0    |  Even\n",
      "  1    |  49   |   1    |  Odd\n",
      "  2    |  50   |   0    |  Even\n",
      "  3    |  51   |   1    |  Odd\n",
      "  4    |  52   |   0    |  Even\n",
      "  5    |  53   |   1    |  Odd\n",
      "  6    |  54   |   0    |  Even\n",
      "  7    |  55   |   1    |  Odd\n",
      "  8    |  56   |   0    |  Even\n",
      "  9    |  57   |   1    |  Odd\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3. Write a Python Program using Perceptron Neural Network to recognise even and odd numbers. Given numbers are in ASCII form 0 to 9  \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare input (one-hot encoding for digits 0 to 9)\n",
    "X = np.eye(10)\n",
    "\n",
    "# Step 2: Labels - 0 for even, 1 for odd\n",
    "y = np.array([0 if i % 2 == 0 else 1 for i in range(10)])\n",
    "\n",
    "# Step 3: Initialize weights and bias\n",
    "weights = np.zeros(10)\n",
    "bias = 0\n",
    "lr = 1\n",
    "epochs = 100\n",
    "\n",
    "# Step 4: Activation function\n",
    "def activation(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "# Step 5: Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        z = np.dot(weights, X[i]) + bias\n",
    "        y_pred = activation(z)\n",
    "        error = y[i] - y_pred\n",
    "        weights += lr * error * X[i]\n",
    "        bias += lr * error\n",
    "\n",
    "# Step 6: Testing and output display\n",
    "print(\"Digit | ASCII | Output | Even/Odd\")\n",
    "print(\"-------------------------------\")\n",
    "for i in range(10):\n",
    "    z = np.dot(weights, X[i]) + bias\n",
    "    output = activation(z)\n",
    "    label = \"Even\" if output == 0 else \"Odd\"\n",
    "    print(f\"  {i}    |  {ord(str(i))}   |   {output}    |  {label}\")\n",
    "\n",
    "\n",
    "''' Here are the answers to your assignment questions in detail:\n",
    "\n",
    "---\n",
    "\n",
    "**Q1. Is it possible to train a NN to distinguish between odd and even numbers only using as input the numbers themselves?**\n",
    "Yes, it is possible to train a neural network (NN) to distinguish between odd and even numbers using the numbers themselves as input. However, the effectiveness depends on how the input is represented. If the input is raw decimal integers (e.g., 1, 2, 3...), the network may not easily learn the pattern. Instead, using binary representations of the numbers helps the NN capture the least significant bit (LSB), which determines whether a number is odd (LSB = 1) or even (LSB = 0). In this case, even a simple feedforward network or a perceptron can learn this classification problem.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. Can Perceptron Generalize Non-linear Problems?**\n",
    "No, a single-layer perceptron cannot generalize non-linear problems. The perceptron can only solve linearly separable problems. For example, it can distinguish AND or OR logic gates but fails on the XOR problem, which is non-linearly separable. To solve non-linear problems, you need a multilayer perceptron (MLP) with non-linear activation functions such as ReLU, Sigmoid, or Tanh.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. How to create a Multilayer Perceptron Neural Network (MLP)?**\n",
    "To create an MLP, follow these steps:\n",
    "\n",
    "1. **Input Layer:** Accepts the input features.\n",
    "2. **Hidden Layers:** One or more layers with neurons that apply a weighted sum and activation function.\n",
    "3. **Activation Functions:** Apply non-linear functions (e.g., ReLU, Tanh) in hidden layers.\n",
    "4. **Output Layer:** Produces the final prediction (e.g., classification label or regression value).\n",
    "5. **Training:** Use backpropagation and gradient descent to minimize the loss function.\n",
    "   You can implement MLP using libraries like TensorFlow or PyTorch. For example, in PyTorch:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # For binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Q4. Is the multilayer perceptron (MLP) a deep learning method? Explain it.**\n",
    "Yes, the multilayer perceptron (MLP) is considered a deep learning method when it contains more than one hidden layer. Deep learning involves neural networks with multiple layers (depth), enabling them to learn complex features and hierarchical patterns. MLPs are foundational models in deep learning and are suitable for tasks like classification, regression, and function approximation. While MLPs are fully connected and not ideal for image or sequence data, they form the basis for more advanced architectures like CNNs and RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "**Q5. What is the best way to fit a large amount of nonlinear data using a neural network?**\n",
    "To fit a large amount of nonlinear data using a neural network:\n",
    "\n",
    "1. **Use Multilayer Architectures:** MLPs with multiple hidden layers can model complex non-linear patterns.\n",
    "2. **Activation Functions:** Apply non-linear functions like ReLU, Tanh, or Sigmoid in hidden layers.\n",
    "3. **Normalization:** Normalize or scale the data for better convergence.\n",
    "4. **Regularization:** Use dropout, L2 regularization, or early stopping to prevent overfitting.\n",
    "5. **Optimizer:** Use advanced optimizers like Adam or RMSProp.\n",
    "6. **Data Augmentation:** Increase the dataset size or diversity if needed.\n",
    "7. **Hyperparameter Tuning:** Adjust the number of neurons, layers, learning rate, etc., for best results.\n",
    "\n",
    "Would you like me to help you generate a PDF or Word version of these answers for submission?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770323df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
